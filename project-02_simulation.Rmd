---
title: "Project 2: Simulation"
author: "Xiangyi Wen; Tanner Huck"
date: ""
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(BSDA)
```

## Part 1: Simulate to answer statistical questions
### Simulation 1
This first simulation is about finding the probability that at least one pair of two people share the same birthday. To do this we can simulate rooms of different amounts of people, assign them a random birthday, and check if anyone has the same birthday. For this simulation we will create rooms of 5,10,15,20,25,30,35,40,45,50 many people 1000 times each.
```{r Birthdays, eval = FALSE}
# creating a variable for the number of replications we are testing for each room size
num_replications <- 1000
# creating a vector for the different room sizes 
room_size <- c(5,10,15,20,25,30,35,40,45,50)

# creating a function that takes an input of the room size, and outputting a Boolean telling us if two people share a  birthday
test_birthday <- function(room_size) {
  test <- vector(length = room_size) # creating a vector of everyone in the room
  for (i in 1:room_size) {
    test[i] <- sample.int(365, 1) # Creating a random birthday for everyone in the room
  }
  return(!length(test) == length(unique(test))) # Testing if anyone has the same birthday as anyone else in the room
}

# make a matrix to hold data from your replications 
replication <- matrix(nrow = num_replications, ncol = 10)
# run simulation 
for (sizeIndex in 1:length(room_size)) {
  for (testNum in 1:num_replications) {
    replication[testNum, sizeIndex] <- test_birthday(room_size[sizeIndex])
  }
}

# make result vector that gives the proportion of shared birthdays for each value of the room size 
result <- vector(length = length(room_size))
for (i in 1:length(room_size)) {
  result[i] <- mean(replication[,i])
}

# make dataframe for plotting that has a variable for sample size n and a variable for 
# proportion of shared birthdays for that value of n 
dataFrameResult <- data.frame(room_size,result)


# plot your results (as a well-labeled ggplot) to look at the relationship between room size and the probability of having a shared birthday 
ggplot(dataFrameResult, aes(room_size, result)) +
  geom_point() +
  labs(title = "Relationship Between Room Size and probability of Having a Shared   Birthday",
       x = "Room Size", y = "Probability of Having a Shared Birthday")
```

From this simulation and graph, we learn that the room size does affect the probability of having a shared birthday. We can see that when the number of people in a room increases, then the probability of two people sharing a birthday also increases. This relationship appears to be mostly linear, with a slight curve at the end. This curve at the end shows that when the room is increasing from around 40 to 50 people, the increase in probability of having a shared birthday is smaller.

### Simulation 2
The second simulation is exploring the relationship between sample size and the probability of a type I error when testing hypothesis'. More specifically, when doing a two-sided t-test with null hypothesis that the population mean is equal to 1, and an alpha level of 0.05. We want to see what the probability is of type I error when doing this t-test with different sample sizes. To do this we will create a simulation with sample sizes of 10,20,30,40,50,60,70,80,90,100 under a normal and exponential distribution of mean 1 and variance 1. We will simulate each sample size 5000 times.
```{r t-tests and type I error}
# creating a vector for the different sample sizes
sample_size <- c(10,20,30,40,50,60,70,80,90,100)
# creating a variable for the number of replications we are testing for each sample size
num_replication <- 5000

# crating matrices to hold the data from the replications 
# matrix for the normal distribution
replication_normal<- matrix(NA,nrow = num_replication,ncol = length(sample_size))
# Matrix for the exponential distribution
replication_expon<- matrix(NA,nrow = num_replication,ncol = length(sample_size))

# creating a function called test_error that will run the simulation for any sample size, it will creates a normal and exponential distribution for that sample size, creates a temporary matrix called result to hold the results, runs a t-test with the distributions and saves the p-value to the result matrix
# inputs: any sample size, creates a normal and exponential distribution for that 
# outputs: returns the result matrix
test_error <- function(sample_size) {
  # creating normal and exponential distributions
  data_normal <- rnorm(sample_size, mean = 1)
  data_expon <- rexp(sample_size, rate = 1)
  # creating matrix called result for the p-values
  result <- matrix(NA,1,2)
  # calculating p-values and testing if they are less than the alpha level, which will return a Boolean true of false. (meaning that there is type I error if the Boolean is   true) then saving it to the result matrix for each distribution
  result[1] <- t.test(data_normal, alternative = "two.sided", mu = 1)$p.value < 0.05
  result[2] <- t.test(data_expon, alternative = "two.sided", mu = 1)$p.value < 0.05
  # returns the result matrix
  return(result)
}

# using a for loop to run the simulation 5000 times and save the results into two larger matrices, one for each type of distribution
for (sizeIndex in 1:length(sample_size)) { # will run for each sample size
  for (testNum in 1:num_replication) { # will run 5000 times
    # uses the test_error function to get the p-values for normal and exponential distributions
    value <- test_error(sample_size[sizeIndex])
    # saves the p-value for the normal distribution to the matrix replication_normal
    replication_normal[testNum, sizeIndex] <- value[1]
    # saves the p-value for the exponential distribution to the matrix replication_expon
    replication_expon[testNum, sizeIndex] <- value[2]
  }
}

# creating a matrix to hold the final results 
result <- matrix(NA, nrow = 2, ncol = length(sample_size))
# for loop to run through each sample size
for (i in 1:length(sample_size)) {
  # calculating the probability of having a type I error for each distribution
  result[1,i] <- mean(replication_normal[,i])
  result[2,i] <- mean(replication_expon[,i])
}

# creating a graph of our results
dataFrameResult <- data.frame(rep(sample_size, 2), c(result[1,], result[2,]), c(rep("normal_distribution", 10), rep("exponential_distribution", 10)))
colnames(dataFrameResult) <- c("sample_size", "probability","distribution_type")
ggplot(dataFrameResult, aes(sample_size, probability, group = distribution_type, color = distribution_type)) +
  geom_point() +
  geom_line() +
  labs(title = "Sample Size and Probability of Type I Error",
       x = "Sample Size", y = "Probability of Having Type I Error")
```

Type I error when hypothesis testing is when you get a false positive. Or in other words, it is when the null hypothesis is true, but you chose to reject it. In our case the null hypothesis is that the population mean is equal to 1. Type I error occurs when we reject the null hypothesis, given that the null hypothesis is true, meaning that the true population mean is equal to 1. When we look at our simulation results, we can see the probability for this error for a normal distribution and exponential distribution with different sample sizes. We can see that for normal distributions, the probability of having this error is low, regardless of the sample size. For exponential distributions, the larger the sample size, the less likely you are to get type I error. 

### Simulation 3

Look at the relationship between the effect size (the magnitude of $\beta$) and the power of a t-test for a coefficient in a linear model. 

Recall that power is the probability of rejecting the null hypothesis given that it is false, so $\text{Power} = P(\text{reject }H_0|H_0\text{ false})$. Consider effect sizes for $\beta$ in the set $\{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}$. Generate each response $Y_i$ from the following linear model:

$$Y_i = 5 + X_i\beta + \epsilon_i$$
$$\epsilon_i\sim N(0,1)$$
Use a sample size of $n = 100$. Use the $X$ vector below. 

```{r}
set.seed(302)
X <- rbinom(100, size = 1, prob = 0.5)
beta <- c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1)
test_power <- function(beta, X) {
  errors <- rnorm(100, mean = 0, sd = 1)
Y <- 5 + X*beta + errors 
df <- data.frame(X = X, 
                 Y = Y)
  
  epsilon <- rnorm(100,0,1)
  Y <- 5 + X*beta + epsilon
  df <- data.frame(X,Y)
  result <- my_lm(X~Y,df)
  return(result)
}

# takes in a formula, and data frame
# returns a table of the coefficient, standard error, t-value and p-value
my_lm <- function(formula, data) {
  X <- model.matrix(formula, data)
  Y <- model.response(model.frame(formula, data))
  lrCoe <- solve( t(X) %*% X ) %*% t(X) %*% Y
  df <- nrow(data) - 2
  sigma_sq <- sum(((Y - X %*% lrCoe) ^2) / df)
  error_lrCoe <- sqrt(diag(sigma_sq * (solve(t(X) %*% X))))
  t_value <- lrCoe / error_lrCoe
  p_value <- 2 * pt(abs(t_value), df, lower.tail = FALSE)
  return(p_value[2] < 0.05)
}

num_replication <- 1000
replication <- matrix(nrow = num_replication, ncol = length(beta))

# run simulation
for (sizeIndex in 1:length(beta)) {
  for (testNum in 1:num_replication) {
    replication[testNum, sizeIndex] <- test_power(beta[sizeIndex], X)
  }
}

# make result vector that gives the proportion of shared birthdays for each value of n
result <- vector(length = length(beta))
for (i in 1:length(beta)) {
  result[i] <- mean(replication[,i])
}

dataFrameResult <- data.frame(beta,result)

ggplot(dataFrameResult, aes(beta, result)) +
  geom_point() +
  geom_line() +
  labs(title = "relationship between the effect size (the magnitude of beta) and the power of a t-test",
       x = "beta", y = "the probability of rejecting the null hypothesis given that it is false")
```



For each effect size (value of $\beta$), simulate data $1000$ times. In your description of your results, be sure to define power. 

## Part 2: Simulate to Teach 

### Simulation 4 

Simulation is often used as a tool to teach statistics. There are many theoretical results that are easier to explain to students through a simulation. Choose a statistical concept, theoretical result, or problem from a homework or exam from a previous statistics class. Design a simulation that would teach this topic or answer this question.

Some example topics include: simple random samples vs convenience samples, the probability of a certain event, the sampling distribution of the sample mean, confidence intervals, the central limit theorem, the difference between median and mode for data from a symmetric distribution vs a skewed distribution, etc. However, feel free to choose your own statistical topic! Please run your chosen topic by me in an email and get it approved before you start this part.

You should write this section as if you are teaching another student. Start with identifying the statistical topic and giving a brief overview. Then, run your simulation. Show your results in a table or plot. Explain your results to the student. Conclude with what you want them to take away from this simulation. 

Part 2 will be worth 9 points. The points will be assigned in the following way. 

* Introducing your topic and giving a brief overview (2.5 points)
* Commenting your code and following the style guidelines (1.5 points) 
* Plotting your results, with a well-labeled plot that follows our visualization style guidelines (1 point) 
* Describing the results (1.5 points)
* Explaining to the student the connection between your statistical problem and your simulation, and using your simulation to guide learning. (2.5 points)
